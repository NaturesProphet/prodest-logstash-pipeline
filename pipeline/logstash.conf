###########################################################################################
# ETAPA DE ENTRADA
###########################################################################################
input 
{
	rabbitmq 
	{
    host => "${RABBIT_HOST}"
		arguments => {"x-message-ttl" => 3600000} # TTL de 1 hora
		queue => "${RABBIT_CONSUMER_QUEUE}"
		subscription_retry_interval_seconds => 5
		exchange => "${RABBIT_EXCHANGE_NAME}"
		exchange_type => "${RABBIT_EXCHANGE_TYPE}"
		ack => true
		prefetch_count => 256
  	}
}
# -----------------------------------------------------------------------------------------
# -----------------------------------------------------------------------------------------
# -----------------------------------------------------------------------------------------
# -----------------------------------------------------------------------------------------


###########################################################################################
# ETAPA DE SAÍDA
# Etapa principal do pipeline, encarregada de alimentar o banco de Histórico no SQL-Server
# e manter atualizado o banco do real-time em MongoDB através de upserts.
# Utiliza os output-plugins 'http' para enviar dados à api responsável pelo MongoDB, o 
# 'jdbc' para alimentar diretamente o SQL-Server, e o 'mongodb' para alimentar diretamente
# o segundo mongo, que armazenará 2 semanas de dados usando TTLs.
###########################################################################################
output 
{
	# -----------------------------------
	mongodb {
		uri => "mongodb://${MONGO2_USER}:${MONGO2_PASS}@${MONGO2_HOST}:${MONGO2_PORT}"
		database => "${MONGO2_DBNAME}"
		collection => "${MONGO2_COLLECTION}"
		isodate => true
	}
# -----------------------------------
	http 
	{
    	http_method => "post"
		url => "http://${MONGO_API_HOST}:${MONGO_API_PORT}"
		message => '{\"IGNICAO\":%{IGNICAO},\"ED1_ACIONADA\":%{ED1_ACIONADA},\"ED2_ACIONADA\":%{ED2_ACIONADA}, \"ED3_ACIONADA\":%{ED3_ACIONADA},\"ED4_ACIONADA\":%{ED4_ACIONADA},\"ROTULO\":\"%{ROTULO}\",\"IDENTIFIER\":%{IDENTIFIER},\"CURSO\":%{CURSO},\"DATAHORA\":\"%{DATAHORA}\",\"TIMESTAMP\":\"%{@timestamp}\",\"MESSAGE_TYPE\":\"%{MESSAGE_TYPE}\",\"LOCALIZACAO\":[%{LONGITUDE},%{LATITUDE}]}'
  	}
# -----------------------------------
	jdbc 
	{
		driver_jar_path => "/usr/share/logstash/mssql-jdbc-7.0.0.jre8.jar"
		connection_string => "jdbc:sqlserver://${MSSQL_HOST}:${MSSQL_PORT};databaseName=${MSSQL_SCHEMA};user=${MSSQL_USER};password=${MSSQL_PASSWORD}"
		statement => [ 
			"INSERT INTO VEICULOS (
						LATITUDE,
						timestamp,
						CURSO,
						LONGITUDE,
						ROTULO,
						DATAHORA,
						IGNICAO
				) 
				VALUES (?,?,?,?,?,?,?)", 
						"LATITUDE",
						"@timestamp",
						"CURSO",
						"LONGITUDE",
						"ROTULO",
						"DATAHORA",
						"IGNICAO"
				]
	}
# -----------------------------------
	elasticsearch 
	{
		hosts => ["${ELASTICSEARCH_SERVER}"]
		index => "veiculos"
		document_type => "GPSReport"
  }

}
# -----------------------------------------------------------------------------------------
# -----------------------------------------------------------------------------------------

